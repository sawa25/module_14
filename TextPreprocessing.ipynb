{"cells":[{"cell_type":"markdown","metadata":{"id":"bXTFLI4NF-LR"},"source":["## Этапы (простой) обработки текста\n","\n","<img src=\"images/textm.png\">"]},{"cell_type":"markdown","metadata":{"id":"rgf1SvTRF-LS"},"source":["\n","## Декодирование\n","\n","\n","**Def.**  \n","перевод последовательности байт в последовательность символов\n","\n","* Распаковка  \n","*plain/.zip/.gz/...*\n","* Кодировка  \n","*ASCII/utf-8/Windows-1251/...*\n","* Формат  \n","*csv/xml/json/doc...*\n","\n","Кроме того: что такое документ?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"I3xU0-PjF-LS"},"source":["## Разбиение на токены\n","**Def.**  \n","разбиение последовательности символов на части (токены), возможно, исключая из рассмотрения некоторые символы  \n","Наивный подход: разделить строку пробелами и выкинуть знаки препинания  \n","\n","\n","*Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.*  \n","\n","\n","**Проблемы:**  \n","* example@example.com, 127.0.0.1\n","* С++, C#\n","* York University vs New York University\n","* Зависимость от языка (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n","Альтернатива: n-граммы"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1719,"status":"ok","timestamp":1631048074715,"user":{"displayName":"Михаил Баранов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVV46Gpmty8si64xPQTsh_aZMsXJnF8ADGg9xOTg=s64","userId":"05195714544608108975"},"user_tz":-180},"id":"BTJ5nrYYF-LT","outputId":"7d3bd7ab-d9ec-4f33-98df-2d1a4688b602"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1631048074720,"user":{"displayName":"Михаил Баранов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVV46Gpmty8si64xPQTsh_aZMsXJnF8ADGg9xOTg=s64","userId":"05195714544608108975"},"user_tz":-180},"id":"Olg2Ah-SF-LV","outputId":"33cd3cf8-dfdf-4f0f-822b-a322096f35b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Трисия\n","любила\n","Нью\n","-\n","Йорк\n",",\n","поскольку\n","любовь\n","к\n","Нью\n","-\n","Йорку\n","могла\n","положительно\n","повлиять\n","на\n","ее\n","карьеру\n",".\n"]}],"source":["from nltk.tokenize import RegexpTokenizer\n","\n","\n","s = \"Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.\"\n","\n","tokenizer = RegexpTokenizer(\"\\w+|[^\\w\\s]+\")\n","for t in tokenizer.tokenize(s): \n","    print(t)"]},{"cell_type":"markdown","metadata":{"id":"buEIu-_CF-LV"},"source":["## Стоп-слова\n","**Def.**  \n","Наиболее частые слова в языке, не содержащие никакой информации о содержании текста\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZL-XwB9F-LW"},"outputs":[],"source":["from nltk.corpus import stopwords\n","\n","\n","print(\" \".join(stopwords.words(\"russian\")[1:20]))"]},{"cell_type":"markdown","metadata":{"id":"aXzzCadWF-LW"},"source":["Проблема: “To be or not to be\""]},{"cell_type":"markdown","metadata":{"id":"M2vMM3qnF-LW"},"source":["## Нормализация\n","**Def.**  \n","Приведение токенов к единому виду для того, чтобы избавиться от поверхностной разницы в написании  \n","\n","Подходы  \n","* сформулировать набор правил, по которым преобразуется токен  \n","Нью-Йорк → нью-йорк → ньюйорк → ньюиорк\n","* явно хранить связи между токенами (WordNet – Princeton)  \n","машина → автомобиль, Windows 6→ window"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INpfX6wqF-LX"},"outputs":[],"source":["s = \"Нью-Йорк\"\n","s1 = s.lower()\n","print(s1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wa4NDjrIF-LX"},"outputs":[],"source":["import re\n","s2 = re.sub(r\"\\W\", \"\", s1, flags=re.U)\n","print(s2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ESECQgj2F-LY"},"outputs":[],"source":["s3 = re.sub(r\"й\", u\"и\", s2, flags=re.U)\n","print(s3)"]},{"cell_type":"markdown","metadata":{"id":"mxgUV4UbF-LY"},"source":["## Стемминг и Лемматизация\n","**Def.**  \n","Приведение грамматических форм слова и однокоренных слов к единой основе (lemma):\n","* Stemming – с помощью простых эвристических правил\n","  * Porter (Cambridge – 1980)\n","        5 этапов, на каждом применяется набор правил, таких как\n","            sses → ss (caresses → caress)\n","            ies → i (ponies → poni)\n","\n","  * Lovins (1968)\n","  * Paice (1990)\n","  * другие\n","* Lemmatization – с использованием словарей и морфологического анализа\n"]},{"cell_type":"markdown","metadata":{"id":"RgDqrYkdF-LZ"},"source":["## Стемминг"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgpAl1MJF-LZ"},"outputs":[],"source":["from nltk.stem.snowball import PorterStemmer\n","from nltk.stem.snowball import RussianStemmer\n","\n","\n","s = PorterStemmer()\n","print(s.stem(\"Tokenization\"))\n","print(s.stem(\"stemming\"))\n","\n","r = RussianStemmer()\n","print(r.stem(\"Авиация\"))\n","print(r.stem(\"национальный\"))"]},{"cell_type":"markdown","metadata":{"id":"xfUJXVE4F-LZ"},"source":["**Наблюдение**  \n","для сложных языков лучше подходит лемматизация"]},{"cell_type":"markdown","metadata":{"id":"6nwjaX0zF-La"},"source":["## Лемматизация"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Bq8bFg_DF-La"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pymorphy2\n","  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n","     -------------------------------------- 55.5/55.5 kB 580.0 kB/s eta 0:00:00\n","Collecting docopt>=0.6\n","  Using cached docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Collecting dawg-python>=0.7.1\n","  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n","Collecting pymorphy2-dicts-ru<3.0,>=2.4\n","  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n","     ---------------------------------------- 8.2/8.2 MB 5.3 MB/s eta 0:00:00\n","Using legacy 'setup.py install' for docopt, since package 'wheel' is not installed.\n","Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n","  Running setup.py install for docopt: started\n","  Running setup.py install for docopt: finished with status 'done'\n","Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install pymorphy2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSekO34BF-La"},"outputs":[],"source":["import pymorphy2\n","\n","\n","morph = pymorphy2.MorphAnalyzer()\n","print(morph.parse(\"думающему\")[0].normal_form)"]},{"cell_type":"markdown","metadata":{"id":"xc6Z0UY9F-La"},"source":["## Heaps' law\n","Эмпирическая закономерность в лингвистике, описывающая распределение числа уникальных слов в документе (или наборе документов) как функцию от его длины.\n","\n","$$\n","M = k T^\\beta, \\;M \\text{ -- размер словаря}, \\; T \\text{ -- количество слов в корпусе}\n","$$\n","$$\n","30 \\leq k \\leq 100, \\; b \\approx 0.5\n","$$\n","\n","<img src=\"images/dim.png\">\n","<img src=\"images/heaps.png\">"]},{"cell_type":"markdown","metadata":{"id":"FzDJlw5OF-Lb"},"source":["## Представление документов\n","**Boolean Model.** Присутствие или отсутствие слова в документе  \n","**Bag of Words.** Порядок токенов не важен  \n","\n","*Погода была ужасная, принцесса была прекрасная.\n","Или все было наоборот?*\n","\n","Координаты\n","* Мультиномиальные: количество токенов в документе\n","* Числовые: взвешенное количество токенов в документе"]},{"cell_type":"markdown","metadata":{"id":"7TlHOiiCF-Lb"},"source":["## Zipf's law\n","Эмпирическая закономерность распределения частоты слов естественного языка\n","\n","$t_1, \\ldots, t_N$ - токены, отранжированные по убыванию частоты\n","   \t\n","$f_1, \\dots, f_N$ - соответствующие частоты\n","\n","**Закон Ципфа**\n","\t$$\n","\tf_i = \\frac{c}{i^k}\n","\t$$\t\n","\t\n","\tЧто еще? Посещаемость сайтов, количество друзей, население городов...\n","<img src=\"images/zipf.png\">\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","s=pd.Series([\"Мама мыла раму мылом\", \"У попа была собака он её любил\"],dtype=str)\n","# s = s.apply(lambda x: x.lower())\n","s=s.str.lower()\n","s=s.str.strip()\n","s\n","nn=set()\n","def growset(x):\n","    for w in x.split():\n","        nn.add(w)\n","s.apply(growset)        \n","df=pd.DataFrame(data=[(list(nn))],columns=[f'col{i}' for i in range(len(nn))])\n","df\n","\n","list(nn)\n","[(list(nn))]\n","s = s.str.split(\" \", expand=True)\n","s\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["ERROR: Could not find a version that satisfies the requirement morpher (from versions: none)\n","ERROR: No matching distribution found for morpher\n"]}],"source":["%pip install morpher \n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import string\n","import pymorphy2\n","morpher = pymorphy2.MorphAnalyzer()\n","\n","def preprocess_txt(line):\n","    sw=[]\n","    # Почистим строку от пунктуации. Для этого пробежимся по каждому символу и проверим, не является ли он знаком пунктуации\n","    exclude = set(string.punctuation)\n","    spls = \"\".join(i for i in line.strip() if i not in exclude).split()\n","    # Лемматизируем все слова в нашем тексте\n","    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]\n","    spls = [i for i in spls if i not in sw and i != \"\"]\n","    return spls"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["0                  [мама, мыло, рама, мыло]\n","1    [у, поп, быть, собака, он, её, любить]\n","dtype: object"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","s = pd.Series([\"Мама мыла раму мылом\", \"У попа была собака он её любил\"], dtype=\"string\")\n","s = s.apply(lambda x: preprocess_txt(x))\n","s"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gensim\n","  Downloading gensim-4.2.0-cp39-cp39-win_amd64.whl (23.9 MB)\n","     ---------------------------------------- 23.9/23.9 MB 4.4 MB/s eta 0:00:00\n","Collecting scipy>=0.18.1\n","  Downloading scipy-1.9.1-cp39-cp39-win_amd64.whl (38.6 MB)\n","     ---------------------------------------- 38.6/38.6 MB 4.1 MB/s eta 0:00:00\n","Collecting numpy>=1.17.0\n","  Downloading numpy-1.23.2-cp39-cp39-win_amd64.whl (14.7 MB)\n","     ---------------------------------------- 14.7/14.7 MB 4.9 MB/s eta 0:00:00\n","Collecting Cython==0.29.28\n","  Downloading Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n","     -------------------------------------- 983.8/983.8 kB 7.8 MB/s eta 0:00:00\n","Collecting smart-open>=1.8.1\n","  Downloading smart_open-6.1.0-py3-none-any.whl (58 kB)\n","     ---------------------------------------- 58.6/58.6 kB 3.2 MB/s eta 0:00:00\n","Installing collected packages: smart-open, numpy, Cython, scipy, gensim\n","Successfully installed Cython-0.29.28 gensim-4.2.0 numpy-1.23.2 scipy-1.9.1 smart-open-6.1.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install gensim"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[==================================================] 100.0% 128.1/128.1MB downloaded\n","cereal\n"]}],"source":["import gensim.downloader as api\n","word_vectors = api.load(\"glove-wiki-gigaword-100\")  # загрузим предтренированные вектора слов из gensim-data\n","# выведем слово наиболее близкое к 'woman', 'king' и далекое от 'man'\n","result = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n","print(word_vectors.doesnt_match(\"breakfast cereal dinner lunch\".split()))"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cereal\n"]}],"source":["print(word_vectors.doesnt_match(\"breakfast cereal dinner lunch\".split()))"]}],"metadata":{"colab":{"name":"TextPreprocessing.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('myenv39')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"ffe9441862ae899b01f43bcb11fbba0096ea954ba725d76e31e738481bf14960"}}},"nbformat":4,"nbformat_minor":0}
